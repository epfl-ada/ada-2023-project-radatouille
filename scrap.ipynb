{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match IMDB, MetaCritic and Freebase IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDS(imdb_ids=[], freebase_ids=[]):\n",
    "    '''\n",
    "        Get the imdb_id, freebase_id and metacritic_id from the wikidata database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imdb_ids : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        imdb_id : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "        metacritic_id : str\n",
    "            The metacritic id of the movie\n",
    "    '''\n",
    "    if len(imdb_ids) > 0:\n",
    "        imdb_ids_string = \" \".join(f'\"{id_}\"' for id_ in imdb_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?imdbId {{ {imdb_ids_string} }}\n",
    "            ?item wdt:P345 ?imdbId .\n",
    "            OPTIONAL {{ ?item wdt:P646 ?freebaseId }}\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    elif len(freebase_ids) > 0:\n",
    "        freebase_ids_string = \" \".join(f'\"{id_}\"' for id_ in freebase_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?freebaseId {{ {freebase_ids_string} }}\n",
    "            ?item wdt:P646 ?freebaseId .\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            OPTIONAL {{ ?item wdt:P345 ?imdbId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    query_url = f\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query={encoded_query}\"\n",
    "\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = response.json()\n",
    "\n",
    "    if len(data[\"results\"][\"bindings\"]) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = data[\"results\"][\"bindings\"]\n",
    "    \n",
    "    imdb_ids = []\n",
    "    freebase_ids = []\n",
    "    metacritic_ids = []\n",
    "\n",
    "    for item in data:\n",
    "        if \"freebaseId\" in item:\n",
    "            freebase_ids.append(item[\"freebaseId\"][\"value\"])\n",
    "        else:\n",
    "            freebase_ids.append(None)\n",
    "        \n",
    "        if \"metacriticId\" in item:\n",
    "            metacritic_ids.append(item[\"metacriticId\"][\"value\"])\n",
    "        else:\n",
    "            metacritic_ids.append(None)\n",
    "\n",
    "        if \"imdbId\" in item:\n",
    "            imdb_ids.append(item[\"imdbId\"][\"value\"])\n",
    "        else:\n",
    "            imdb_ids.append(None)\n",
    "\n",
    "    return imdb_ids, freebase_ids, metacritic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_imdb_and_metacritics_ids(movies, batch_size=100):\n",
    "\n",
    "    new_movies = movies.copy()\n",
    "\n",
    "    # Initializing imdb_id and metacritic_id columns if they don't exist\n",
    "    if 'imdb_id' not in new_movies.columns:\n",
    "        new_movies['imdb_id'] = None\n",
    "    if 'metacritic_id' not in new_movies.columns:\n",
    "        new_movies['metacritic_id'] = None\n",
    "\n",
    "    for i in tqdm(range(0, len(new_movies), batch_size)):\n",
    "        batch = new_movies.iloc[i:i+batch_size]\n",
    "\n",
    "        # drop columns imdb_id and metacritic_id\n",
    "        batch = batch.drop(columns=[\"imdb_id\", \"metacritic_id\"])\n",
    "\n",
    "        imdb_ids, freebase_ids, metacritic_ids = get_IDS(freebase_ids=batch[\"freebase_id\"].values)\n",
    "\n",
    "        ids_mapping = pd.DataFrame({\"freebase_id\": freebase_ids, \"imdb_id\": imdb_ids, \"metacritic_id\": metacritic_ids})\n",
    "\n",
    "        # if duplicates then set imdb and metacritic ids to None\n",
    "        duplicates = ids_mapping[\"freebase_id\"].duplicated(keep=False)\n",
    "        ids_mapping.loc[duplicates, [\"imdb_id\", \"metacritic_id\"]] = None\n",
    "\n",
    "        # removing duplicates\n",
    "        ids_mapping = ids_mapping.drop_duplicates(subset=[\"freebase_id\"])\n",
    "\n",
    "        # Ensuring one-to-one correspondence\n",
    "        if not ids_mapping[\"freebase_id\"].is_unique:\n",
    "            # print duplicates\n",
    "            raise ValueError(\"Duplicate freebase_ids found in ids_mapping.\")\n",
    "\n",
    "        # Merging and updating the DataFrame\n",
    "        batch_updated = batch.merge(ids_mapping, on=\"freebase_id\", how=\"left\")\n",
    "        new_movies.iloc[i:i+batch_size] = batch_updated\n",
    "\n",
    "    return new_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/preprocessed/movie.metadata.preprocessed.tsv', sep='\\t')\n",
    "\n",
    "cmu_movies = add_imdb_and_metacritics_ids(movies, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_movies.to_csv('data/processed/cmu_movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacritic Critics Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics_movie(metacriticID):\n",
    "\n",
    "    url = f\"https://www.metacritic.com/{metacriticID}/critic-reviews\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    script_tag = soup.find('script', string=lambda t: t and 'window.__NUXT__' in t)\n",
    "\n",
    "    # Define a regular expression pattern to match objects with specific attributes\n",
    "    pattern = r\"\\{[^{}]*reviewedProduct:\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}[^{}]*\\}\"\n",
    "\n",
    "    if not script_tag:\n",
    "        return None\n",
    "    \n",
    "    # Find all matches of the pattern in the input text\n",
    "    matches = re.findall(pattern, script_tag.text, re.DOTALL)\n",
    "    \n",
    "    def extract_info(data_string):\n",
    "        \n",
    "        # Regular expressions for score (next to metaScore), author, and publicationName\n",
    "        metascore_pattern = r\"criticScoreSummary:\\{[^\\}]*score:(\\d+)\"\n",
    "        score_pattern = r'score:([a-zA-Z]|\\d+),\\s*metaScore'\n",
    "        author_pattern = r'author:\"([^\"]+)\"'\n",
    "        publication_name_pattern = r'publicationName:\"([^\"]+)\"'\n",
    "\n",
    "        # Extracting score\n",
    "        score_match = re.search(score_pattern, data_string)\n",
    "        if score_match:\n",
    "            score = score_match.group(1)\n",
    "            if score.isalpha():\n",
    "                score = 0\n",
    "            else:\n",
    "                score = int(score)\n",
    "        else:\n",
    "            score = None\n",
    "\n",
    "        # Extracting author\n",
    "        author_match = re.search(author_pattern, data_string)\n",
    "        author = author_match.group(1) if author_match else None\n",
    "\n",
    "        # Extracting publicationName\n",
    "        publication_name_match = re.search(publication_name_pattern, data_string)\n",
    "        publication_name = publication_name_match.group(1) if publication_name_match else None\n",
    "\n",
    "        # Extracting metascore\n",
    "        metascore_match = re.search(metascore_pattern, data_string)\n",
    "        metascore = metascore_match.group(1) if metascore_match else None\n",
    "\n",
    "        return score, author, publication_name, metascore\n",
    "        \n",
    "    reviews_data = {\"publisher\": [], \"author\": [], \"metacritic_rating\": [], \"metascore\": []}\n",
    "    \n",
    "    for review in matches:\n",
    "\n",
    "        score, author, publisher, metascore = extract_info(review)\n",
    "\n",
    "        reviews_data[\"publisher\"].append(publisher)\n",
    "        reviews_data[\"author\"].append(author)\n",
    "        reviews_data[\"metacritic_rating\"].append(score)\n",
    "        reviews_data[\"metascore\"].append(metascore)\n",
    "\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics(movies_df, save_step=250, filepath=\"data/external/metacritic_reviews.csv\"):\n",
    "    if not os.path.exists(filepath):\n",
    "        metacritic_reviews = pd.DataFrame(columns=[\"publisher\", \"author\", \"metacritic_rating\", \"metascore\", \"metacritic_id\"])\n",
    "    else:\n",
    "        metacritic_reviews = pd.read_csv(filepath)\n",
    "\n",
    "    already_scraped_movies_ids = metacritic_reviews[\"metacritic_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    \n",
    "    movies_df = movies_df.loc[~movies_df[\"metacritic_id\"].isin(already_scraped_movies_ids)]\n",
    "\n",
    "    movies_df = movies_df.loc[movies_df[\"metacritic_id\"].notna()]\n",
    "\n",
    "    initial_len = len(metacritic_reviews)\n",
    "\n",
    "    for i, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        metacritic_id = row[\"metacritic_id\"]\n",
    "\n",
    "        if not metacritic_id:\n",
    "            continue\n",
    "\n",
    "        reviews = scrap_metacritics_movie(metacritic_id)\n",
    "\n",
    "        if reviews is None:\n",
    "            continue\n",
    "\n",
    "        reviews[\"metacritic_id\"] = row[\"metacritic_id\"]\n",
    "\n",
    "        metacritic_reviews = pd.concat([metacritic_reviews, reviews], ignore_index=True)\n",
    "\n",
    "        if i % save_step == 0:\n",
    "            metacritic_reviews.to_csv(filepath, index=False)\n",
    "            print(\"Saved {} new reviews\".format(len(metacritic_reviews) - initial_len))\n",
    "            initial_len = len(metacritic_reviews)\n",
    "\n",
    "    metacritic_reviews.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/8529 [00:00<39:18,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/8529 [00:31<2:28:14,  1.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m movies \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mdata/processed/cmu_movies.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scrap_metacritics(movies, save_step\u001b[39m=\u001b[39;49m\u001b[39m250\u001b[39;49m, filepath\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdata/external/metacritic_reviews.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m metacritic_id:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m reviews \u001b[39m=\u001b[39m scrap_metacritics_movie(metacritic_id)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m reviews \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;32m/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://www.metacritic.com/\u001b[39m\u001b[39m{\u001b[39;00mmetacriticID\u001b[39m}\u001b[39;00m\u001b[39m/critic-reviews\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/antoninfaure/Documents/MA3/ada-2023-project-radatouille/scrap.ipynb#Y652sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m script_tag \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mscript\u001b[39m\u001b[39m'\u001b[39m, string\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m t: t \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mwindow.__NUXT__\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m t)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    582\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    583\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    584\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    585\u001b[0m }\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/sessions.py:723\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m allow_redirects:\n\u001b[1;32m    721\u001b[0m     \u001b[39m# Redirect resolving generator.\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     gen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresolve_redirects(r, request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 723\u001b[0m     history \u001b[39m=\u001b[39m [resp \u001b[39mfor\u001b[39;00m resp \u001b[39min\u001b[39;00m gen]\n\u001b[1;32m    724\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     history \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/sessions.py:266\u001b[0m, in \u001b[0;36mSessionRedirectMixin.resolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[39myield\u001b[39;00m req\n\u001b[1;32m    264\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 266\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    267\u001b[0m         req,\n\u001b[1;32m    268\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    269\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    270\u001b[0m         verify\u001b[39m=\u001b[39;49mverify,\n\u001b[1;32m    271\u001b[0m         cert\u001b[39m=\u001b[39;49mcert,\n\u001b[1;32m    272\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    273\u001b[0m         allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49madapter_kwargs,\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m     extract_cookies_to_jar(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcookies, prepared_request, resp\u001b[39m.\u001b[39mraw)\n\u001b[1;32m    279\u001b[0m     \u001b[39m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    700\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    703\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    704\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/requests/adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 489\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    490\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    491\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    492\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    493\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    494\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    496\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    497\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    498\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    499\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    500\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.12_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "movies = pd.read_csv(\"data/processed/cmu_movies.csv\")\n",
    "\n",
    "scrap_metacritics(movies, save_step=250, filepath=\"data/external/metacritic_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards_nominations_batch(freebase_ids):\n",
    "    formatted_ids = ' '.join(f'\"{id_}\"' for id_ in freebase_ids)\n",
    "    sparql_query_awards = f\"\"\"\n",
    "    SELECT ?item ?movieLabel ?movieFreebaseID ?awardLabel WHERE {{\n",
    "    VALUES ?movieFreebaseID {{ {formatted_ids} }}\n",
    "    ?item wdt:P646 ?movieFreebaseID .\n",
    "    OPTIONAL {{ \n",
    "      ?item wdt:P166 ?award .\n",
    "      ?award rdfs:label ?awardLabel .\n",
    "      FILTER(LANG(?awardLabel) = \"en\")\n",
    "    }}\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "  }}\n",
    "    \"\"\"\n",
    "    encoded_query_awards = urllib.parse.quote(sparql_query_awards)\n",
    "\n",
    "    sparql_query_nominations = f\"\"\"\n",
    "    SELECT ?item ?movieLabel ?movieFreebaseID ?nominationLabel WHERE {{\n",
    "    VALUES ?movieFreebaseID {{ {formatted_ids} }}\n",
    "    ?item wdt:P646 ?movieFreebaseID .\n",
    "    OPTIONAL {{ \n",
    "      ?item wdt:P1411 ?nomination .\n",
    "      ?nomination rdfs:label ?nominationLabel .\n",
    "      FILTER(LANG(?nominationLabel) = \"en\")\n",
    "    }}\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "  }}\n",
    "    \"\"\"\n",
    "    encoded_query_nominations = urllib.parse.quote(sparql_query_nominations)\n",
    "\n",
    "    url = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query={}\"\n",
    "    url_awards = url.format(encoded_query_awards)\n",
    "    url_nominations = url.format(encoded_query_nominations)\n",
    "    \n",
    "    # Fetch awards\n",
    "    response_awards = requests.get(url_awards)\n",
    "    if response_awards.status_code == 200:\n",
    "      data_awards = response_awards.json()\n",
    "\n",
    "      results_awards = [{\n",
    "          'freebase_id': item['movieFreebaseID']['value'],\n",
    "          'type': 'award',\n",
    "          'name': item['awardLabel']['value']\n",
    "      } for item in data_awards['results']['bindings'] if 'awardLabel' in item]\n",
    "    else:\n",
    "      results_awards = []\n",
    "\n",
    "    # Fetch nominations\n",
    "    response_nominations = requests.get(url_nominations)\n",
    "\n",
    "    if response_nominations.status_code == 200:\n",
    "      data_nominations = response_nominations.json()\n",
    "      results_nominations = [{\n",
    "          'freebase_id': item['movieFreebaseID']['value'],\n",
    "          'type': 'nomination',\n",
    "          'name': item['nominationLabel']['value']\n",
    "      } for item in data_nominations['results']['bindings'] if 'nominationLabel' in item]\n",
    "    else:\n",
    "      results_nominations = []\n",
    "\n",
    "    # Combine results and create DataFrame\n",
    "    combined_results = results_awards + results_nominations\n",
    "    return pd.DataFrame(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards_nominations(movies, batch_size=250):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(movies), batch_size), total=len(movies)//batch_size):\n",
    "        batch = movies[i:i+batch_size]\n",
    "        freebase_ids = batch['freebase_id'].tolist()\n",
    "        results.append(get_awards_nominations_batch(freebase_ids))\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - IMDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_awards_movie(metacriticID):\n",
    "    url = 'https://www.imdb.com/title/{}/awards/'.format(metacriticID)    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    div = soup.find_all('div', attrs={'data-testid':\"awards-signpost\"})\n",
    "    if len(div) == 0:\n",
    "        return 0, 0\n",
    "    awards = div[0].find_all('div', class_=\"ipc-signpost__text\")[0].get_text(strip=True)\n",
    "    # extract the number of awards\n",
    "    # Structure of the text: \"N wins & M nominations.\"\n",
    "    wins = 0\n",
    "    nominations = 0\n",
    "    if len(awards.split('&')) == 2:\n",
    "        wins = int(awards.split('&')[0].split()[0])\n",
    "        nominations = int(awards.split('&')[1].split()[0])\n",
    "    elif \"wins\" in awards:\n",
    "        wins = int(awards.split()[0])\n",
    "    elif \"nominations\" in awards:\n",
    "        nominations = int(awards.split()[0])\n",
    "    return wins, nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_awards(movies_df, save_step=250):\n",
    "    if not os.path.exists(\"data/external/imdb_awards.csv\"):\n",
    "        imdb_awards = pd.DataFrame(columns=[\"freebase_id\", \"nominations\", \"wins\"])\n",
    "    else:\n",
    "        imdb_awards = pd.read_csv(\"data/external/imdb_awards.csv\")\n",
    "\n",
    "    already_scraped_movies_ids = imdb_awards[\"freebase_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    movies_df = movies_df.loc[~movies_df[\"freebase_id\"].isin(already_scraped_movies_ids)].reset_index(drop=True)\n",
    "    \n",
    "    initial_len = len(imdb_awards)\n",
    "\n",
    "    for i, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        imdb_id = row[\"imdb_id\"]\n",
    "        if not imdb_id:\n",
    "            print(row)\n",
    "            continue\n",
    "\n",
    "        wins, nominations = scrap_awards_movie(imdb_id)\n",
    "\n",
    "        award = pd.DataFrame({\"freebase_id\": [row[\"freebase_id\"]], \"nominations\": [nominations], \"wins\": [wins]})\n",
    "\n",
    "        imdb_awards = pd.concat([imdb_awards, award], ignore_index=True)\n",
    "        if i % save_step == 0:\n",
    "            imdb_awards.to_csv(\"data/external/imdb_awards.csv\", index=False)\n",
    "            print(\"Saved {} new awards\".format(len(imdb_awards) - initial_len))\n",
    "            initial_len = len(imdb_awards)\n",
    "\n",
    "    imdb_awards.to_csv(\"data/external/imdb_awards.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"data/processed/cmu_movies.csv\", sep=\"\\t\")\n",
    "movies = movies.loc[(movies['freebase_id'].notnull()) & (movies['imdb_id'].notnull()) & (movies['metacritic_id'].notnull())] \n",
    "scrap_awards(movies, save_step=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
