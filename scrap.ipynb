{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDS(imdb_id=None, freebase_id=None):\n",
    "    '''\n",
    "        Get the imdb_id, freebase_id and metacritic_id from the wikidata database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imdb_id : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        imdb_id : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "        metacritic_id : str\n",
    "            The metacritic id of the movie\n",
    "    '''\n",
    "    if imdb_id:\n",
    "        query_url = f\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=SELECT%20%3Fitem%20%3FfreebaseId%20%3FmetacriticId%20%3FimdbId%20WHERE%20%7B%0A%20%20%20%20%3Fitem%20wdt%3AP345%20%22{imdb_id}%22%20.%0A%20%20%20%20OPTIONAL%20%7B%20%3Fitem%20wdt%3AP646%20%3FfreebaseId%20%7D%0A%20%20%20%20OPTIONAL%20%7B%20%3Fitem%20wdt%3AP1712%20%3FmetacriticId%20%7D%0A%20%20%20%20OPTIONAL%20%7B%20%3Fitem%20wdt%3AP345%20%3FimdbId%20%7D%0A%7D\"\n",
    "    elif freebase_id:\n",
    "        query_url = f\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=SELECT%20%3Fitem%20%3FfreebaseId%20%3FmetacriticId%20%3FimdbId%20WHERE%20%7B%0A%20%20%20%20%3Fitem%20wdt%3AP646%20%22{freebase_id}%22%20.%0A%20%20%20%20OPTIONAL%20%7B%20%3Fitem%20wdt%3AP1712%20%3FmetacriticId%20%7D%0A%20%20%20%20OPTIONAL%20%7B%20%3Fitem%20wdt%3AP345%20%3FimdbId%20%7D%0A%7D\"\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "\n",
    "    if len(data[\"results\"][\"bindings\"]) == 0:\n",
    "        return None\n",
    "    \n",
    "    data = data[\"results\"][\"bindings\"][0]\n",
    "    \n",
    "    if \"freebaseId\" in data:\n",
    "        freebase_id = data[\"freebaseId\"][\"value\"]\n",
    "    else:\n",
    "        freebase_id = None\n",
    "    \n",
    "    if \"metacriticId\" in data:\n",
    "        metacritic_id = data[\"metacriticId\"][\"value\"]\n",
    "    else:\n",
    "        metacritic_id = None\n",
    "\n",
    "    if \"imdbId\" in data:\n",
    "        imdb_id = data[\"imdbId\"][\"value\"]\n",
    "    else:\n",
    "        imdb_id = None\n",
    "\n",
    "    return imdb_id, freebase_id, metacritic_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movies():\n",
    "    url = \"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&start={start}\"\n",
    "\n",
    "    data = {\n",
    "        \"imdb_id\": [], \n",
    "        \"title\": [],\n",
    "        \"year\": [],\n",
    "        \"duration\": [],\n",
    "        \"genres\": [],\n",
    "        \"number_of_ratings\": [],\n",
    "        \"rating\": [],\n",
    "        \"director\": [],\n",
    "        \"description\": [],\n",
    "        \"freebase_id\": [],\n",
    "        \"metacritic_id\": [],\n",
    "        \"metascore\": [],\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(total=1000)\n",
    "\n",
    "    while True:\n",
    "        start = len(data[\"title\"]) + 1\n",
    "\n",
    "        response = requests.get(url.format(start=start))\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        movies = soup.find_all(class_=\"lister-item-content\")\n",
    "\n",
    "        if not movies or len(movies) == 0:\n",
    "            break\n",
    "\n",
    "        for movie in movies:\n",
    "            pbar.update(1)\n",
    "            imdb_id = movie.find(\"a\")[\"href\"].split(\"/\")[2]\n",
    "            data[\"imdb_id\"].append(imdb_id)\n",
    "            data[\"title\"].append(movie.find(\"a\").get_text())\n",
    "\n",
    "            year_text = movie.find(class_=\"lister-item-year\").get_text().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            year_match = re.search(r'\\d{4}', year_text)\n",
    "\n",
    "            if year_match:\n",
    "                year = year_match.group()\n",
    "            else:\n",
    "                year = None\n",
    "        \n",
    "            data[\"year\"].append(year)\n",
    "            data[\"duration\"].append(movie.find(class_=\"runtime\").get_text())\n",
    "            data[\"genres\"].append(movie.find(class_=\"genre\").get_text().strip())\n",
    "            data[\"number_of_ratings\"].append(\n",
    "                movie.find(class_=\"sort-num_votes-visible\").find_all(\"span\")[1].get_text()\n",
    "            )\n",
    "            data[\"rating\"].append(movie.find(class_=\"ratings-imdb-rating\").get_text().strip())\n",
    "            data[\"director\"].append(movie.find_all(\"p\")[2].find(\"a\").get_text())\n",
    "            data[\"description\"].append(movie.find_all(\"p\", class_=\"text-muted\")[-1].get_text().strip())\n",
    "            metascore = movie.find(\"span\", class_=\"metascore\")\n",
    "            if metascore:\n",
    "                data[\"metascore\"].append(int(metascore.get_text().strip()))\n",
    "            else:\n",
    "                data[\"metascore\"].append(None)\n",
    "\n",
    "            _, freebase_id, metacritic_id = get_IDS(imdb_id=imdb_id)\n",
    "\n",
    "            data[\"freebase_id\"].append(freebase_id)\n",
    "            data[\"metacritic_id\"].append(metacritic_id)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "    df['number_of_ratings'] = df['number_of_ratings'].str.replace(',', '').astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c93da8dd65e4ed8a9fd5ac2c96f715a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "movies = scrap_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:2540: RuntimeWarning: invalid value encountered in cast\n",
      "  values = values.astype(str)\n"
     ]
    }
   ],
   "source": [
    "movies.to_csv(\"data/scrap/top1000_IMDB_movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_reviews(movieID, max_pages=None):\n",
    "    url = (\n",
    "        \"https://www.imdb.com/title/{movieID}/reviews/_ajax?paginationKey={}\"\n",
    "    )\n",
    "    key = \"\"\n",
    "    data = {\"imdb_id\": [], \"review\": [], \"rating\": [], \"date\": [], \"user\": []}\n",
    "\n",
    "    response = requests.get(\"https://www.imdb.com/title/{movieID}/reviews\".format(movieID = movieID))\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    total_reviews = int(soup.find(class_=\"lister\").find(class_=\"header\").find(\"span\").get_text().split()[0].replace(',', ''))\n",
    "    \n",
    "    pbar2 = tqdm(total=total_reviews, position=1, leave=True)\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        if max_pages and i > max_pages:\n",
    "            break\n",
    "        response = requests.get(url.format(key, movieID = movieID))\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        for review, rating, date, user in zip(\n",
    "            soup.find_all(class_=\"text show-more__control\"), soup.find_all(class_=\"rating-other-user-rating\"), soup.find_all(class_=\"review-date\"), soup.find_all(class_=\"display-name-link\")\n",
    "        ):\n",
    "            data[\"imdb_id\"].append(movieID)\n",
    "            data[\"review\"].append(review.get_text())\n",
    "            data[\"rating\"].append(rating.find(\"span\").get_text(strip=True))\n",
    "            data[\"date\"].append(date.get_text(strip=True))\n",
    "            data[\"user\"].append(user.find(\"a\")[\"href\"].split(\"/\")[2])\n",
    "\n",
    "        # Find the pagination key\n",
    "        pagination_key = soup.find(\"div\", class_=\"load-more-data\")\n",
    "        if not pagination_key:\n",
    "            break\n",
    "        \n",
    "        # Update the `key` variable in-order to scrape more reviews\n",
    "        key = pagination_key[\"data-key\"]\n",
    "        \n",
    "        pbar2.update(25)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['rating'] = df['rating'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_reviews(movies, max_pages=None, reviews_threshold=10000):\n",
    "    reviews = pd.DataFrame()\n",
    "    \n",
    "    files = os.listdir(\"data/reviews\")\n",
    "\n",
    "    already_scraped_movies = pd.Series()\n",
    "\n",
    "    if len(files) > 0:\n",
    "        for file in files:\n",
    "            file_reviews = pd.read_csv(\"data/reviews/{}\".format(file))\n",
    "            already_scraped_movies = pd.concat([already_scraped_movies, file_reviews['movie']])\n",
    "\n",
    "    already_scraped_movies = already_scraped_movies.unique()\n",
    "\n",
    "    print(\"Already scraped {}/{} movies\".format(len(already_scraped_movies), len(movies)))\n",
    "\n",
    "    movies = movies[~movies[\"id\"].isin(already_scraped_movies)]\n",
    "\n",
    "    if len(movies) == 0:\n",
    "        print(\"All movies have already been scraped\")\n",
    "        return None\n",
    "\n",
    "    pbar1 = tqdm(total=len(movies), position=0, leave=True)\n",
    "    \n",
    "\n",
    "    for movieID in movies[\"imdb_id\"]:\n",
    "\n",
    "        print('Scraping reviews for movie {}'.format(movies[movies['imdb_id'] == movieID]['title'].values[0]))\n",
    "\n",
    "        reviews = pd.concat([reviews, scrap_movie_reviews(movieID, max_pages=max_pages)])\n",
    "\n",
    "        if len(reviews) > reviews_threshold:\n",
    "            # create a file \"review\" + i + \".csv\" with i the number of files in the reviews folder\n",
    "            files = os.listdir(\"data/reviews\")\n",
    "            file_number = len(files) + 1\n",
    "            reviews.to_csv(\"data/reviews/reviews{}.csv\".format(file_number), index=False)\n",
    "            \n",
    "            print(\"Scraped {} reviews\".format(len(reviews)))\n",
    "            print(\"Saved to data/reviews/reviews{}.csv\".format(file_number))\n",
    "            \n",
    "            # reset the reviews dataframe\n",
    "            reviews = pd.DataFrame()\n",
    "        pbar1.update(1)\n",
    "\n",
    "    # append the reviews to the csv file\n",
    "    if len(reviews) > 0:\n",
    "        files = os.listdir(\"data/reviews\")\n",
    "        file_number = len(files) + 1\n",
    "        reviews.to_csv(\"data/reviews/reviews{}.csv\".format(file_number), index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_reviews(movies, max_pages=None, reviews_threshold=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_cast(movieID):\n",
    "    url = f\"https://www.imdb.com/title/{movieID}/fullcredits\"\n",
    "\n",
    "    data = {\"imdb_id\": [], \"actor\": [], \"character\": []}\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    castlist = soup.find_all(\"table\", class_=\"cast_list\")[0].find_all(\"tr\")\n",
    "\n",
    "    castlist = castlist[1:]\n",
    "\n",
    "    for cast in castlist:\n",
    "        if not(\"odd\" in cast.attrs.get(\"class\", []) or \"even\" in cast.attrs.get(\"class\", [])):\n",
    "            break\n",
    "\n",
    "        data[\"imdb_id\"].append(movieID)\n",
    "        data[\"actor\"].append(cast.find_all(\"td\")[1].find(\"a\").get_text(strip=True))\n",
    "        data[\"character\"].append(cast.find(\"td\", class_=\"character\").get_text(strip=True))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_casts(movies):\n",
    "    casts = pd.DataFrame()\n",
    "\n",
    "    for movieID in tqdm(movies[\"imdb_id\"]):\n",
    "        casts = pd.concat([casts, scrap_movie_cast(movieID)])\n",
    "    return casts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts = scrap_casts(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts.to_csv(\"data/casts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_summary(imdb_id):\n",
    "    url = f\"https://www.imdb.com/title/{imdb_id}/plotsummary\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    synopsis = soup.find_all('section', class_=\"ipc-page-section ipc-page-section--base\")[1].find(class_=\"ipc-metadata-list-item__content-container\")\n",
    "    if synopsis:\n",
    "        synopsis = synopsis.get_text(strip=True)\n",
    "\n",
    "    return synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_summaries(movies):\n",
    "    summaries = {\n",
    "        \"imdb_id\": [],\n",
    "        \"summary\": []\n",
    "    }\n",
    "\n",
    "    for imdb_id in tqdm(movies[\"imdb_id\"]):\n",
    "        summary = scrap_movie_summary(imdb_id)\n",
    "\n",
    "        if summary:\n",
    "            summaries[\"imdb_id\"].append(imdb_id)\n",
    "            summaries[\"summary\"].append(summary)\n",
    "\n",
    "    df = pd.DataFrame(summaries)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = scrap_summaries(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.to_csv(\"data/summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critics Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics_movie(metacriticID):\n",
    "\n",
    "    url = f\"https://www.metacritic.com/{metacriticID}/critic-reviews\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    script_tag = soup.find('script', text=lambda t: t and 'window.__NUXT__' in t)\n",
    "\n",
    "    # Define a regular expression pattern to match objects with specific attributes\n",
    "    pattern = r\"\\{[^{}]*reviewedProduct:\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}[^{}]*\\}\"\n",
    "\n",
    "    # Find all matches of the pattern in the input text\n",
    "    matches = re.findall(pattern, script_tag.text, re.DOTALL)\n",
    "    \n",
    "    def extract_info(data_string):\n",
    "        \n",
    "        # Regular expressions for score (next to metaScore), author, and publicationName\n",
    "        score_pattern = r'score:([a-zA-Z]|\\d+),\\s*metaScore'\n",
    "        author_pattern = r'author:\"([^\"]+)\"'\n",
    "        publication_name_pattern = r'publicationName:\"([^\"]+)\"'\n",
    "\n",
    "        # Extracting score\n",
    "        score_match = re.search(score_pattern, data_string)\n",
    "        if score_match:\n",
    "            score = score_match.group(1)\n",
    "            if score.isalpha():\n",
    "                score = 0\n",
    "            else:\n",
    "                score = int(score)\n",
    "        else:\n",
    "            score = None\n",
    "\n",
    "        # Extracting author\n",
    "        author_match = re.search(author_pattern, data_string)\n",
    "        author = author_match.group(1) if author_match else None\n",
    "\n",
    "        # Extracting publicationName\n",
    "        publication_name_match = re.search(publication_name_pattern, data_string)\n",
    "        publication_name = publication_name_match.group(1) if publication_name_match else None\n",
    "\n",
    "        return score, author, publication_name\n",
    "        \n",
    "    reviews_data = {\"publisher\": [], \"author\": [], \"rating\": []}\n",
    "    \n",
    "    for review in matches:\n",
    "\n",
    "        score, author, publisher = extract_info(review)\n",
    "\n",
    "        reviews_data[\"publisher\"].append(publisher)\n",
    "        reviews_data[\"author\"].append(author)\n",
    "        reviews_data[\"rating\"].append(score)\n",
    "\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics(movies_df):\n",
    "    if not os.path.exists(\"data/scrap/metacritic_reviews.csv\"):\n",
    "        metacritic_reviews = pd.DataFrame(columns=[\"publisher\", \"author\", \"rating\", \"imdb_id\", \"metacritic_id\"])\n",
    "    else:\n",
    "        metacritic_reviews = pd.read_csv(\"data/scrap/metacritic_reviews.csv\")\n",
    "\n",
    "    already_scraped_movies_ids = metacritic_reviews[\"imdb_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    \n",
    "    movies_df = movies_df.loc[~movies_df[\"imdb_id\"].isin(already_scraped_movies_ids)]\n",
    "\n",
    "    for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        metacritic_id = row[\"metacritic_id\"]\n",
    "\n",
    "        if not metacritic_id:\n",
    "            continue\n",
    "\n",
    "        reviews = scrap_metacritics_movie(metacritic_id)\n",
    "\n",
    "        reviews[\"imdb_id\"] = row[\"imdb_id\"]\n",
    "        reviews[\"metacritic_id\"] = row[\"metacritic_id\"]\n",
    "\n",
    "        metacritic_reviews = pd.concat([metacritic_reviews, reviews], ignore_index=True)\n",
    "\n",
    "    metacritic_reviews.to_csv(\"data/scrap/metacritic_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"data/scrap/top1000_IMDB_movies.csv\")\n",
    "\n",
    "scrap_metacritics(movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
