{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_403/1688192513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match IMDB, MetaCritic and Freebase IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDS(imdb_ids=[], freebase_ids=[]):\n",
    "    '''\n",
    "        Get the imdb_id, freebase_id and metacritic_id from the wikidata database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imdb_ids : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        imdb_id : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "        metacritic_id : str\n",
    "            The metacritic id of the movie\n",
    "    '''\n",
    "    if len(imdb_ids) > 0:\n",
    "        imdb_ids_string = \" \".join(f'\"{id_}\"' for id_ in imdb_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?imdbId {{ {imdb_ids_string} }}\n",
    "            ?item wdt:P345 ?imdbId .\n",
    "            OPTIONAL {{ ?item wdt:P646 ?freebaseId }}\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    elif len(freebase_ids) > 0:\n",
    "        freebase_ids_string = \" \".join(f'\"{id_}\"' for id_ in freebase_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?freebaseId {{ {freebase_ids_string} }}\n",
    "            ?item wdt:P646 ?freebaseId .\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            OPTIONAL {{ ?item wdt:P345 ?imdbId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    query_url = f\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query={encoded_query}\"\n",
    "\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = response.json()\n",
    "\n",
    "    if len(data[\"results\"][\"bindings\"]) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = data[\"results\"][\"bindings\"]\n",
    "    \n",
    "    imdb_ids = []\n",
    "    freebase_ids = []\n",
    "    metacritic_ids = []\n",
    "\n",
    "    for item in data:\n",
    "        if \"freebaseId\" in item:\n",
    "            freebase_ids.append(item[\"freebaseId\"][\"value\"])\n",
    "        else:\n",
    "            freebase_ids.append(None)\n",
    "        \n",
    "        if \"metacriticId\" in item:\n",
    "            metacritic_ids.append(item[\"metacriticId\"][\"value\"])\n",
    "        else:\n",
    "            metacritic_ids.append(None)\n",
    "\n",
    "        if \"imdbId\" in item:\n",
    "            imdb_ids.append(item[\"imdbId\"][\"value\"])\n",
    "        else:\n",
    "            imdb_ids.append(None)\n",
    "\n",
    "    return imdb_ids, freebase_ids, metacritic_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_imdb_and_metacritics_ids(movies, batch_size=100):\n",
    "\n",
    "    new_movies = movies.copy()\n",
    "\n",
    "    # Initializing imdb_id and metacritic_id columns if they don't exist\n",
    "    if 'imdb_id' not in new_movies.columns:\n",
    "        new_movies['imdb_id'] = None\n",
    "    if 'metacritic_id' not in new_movies.columns:\n",
    "        new_movies['metacritic_id'] = None\n",
    "\n",
    "    for i in tqdm(range(0, len(new_movies), batch_size)):\n",
    "        batch = new_movies.iloc[i:i+batch_size]\n",
    "\n",
    "        # drop columns imdb_id and metacritic_id\n",
    "        batch = batch.drop(columns=[\"imdb_id\", \"metacritic_id\"])\n",
    "\n",
    "        imdb_ids, freebase_ids, metacritic_ids = get_IDS(freebase_ids=batch[\"freebase_id\"].values)\n",
    "\n",
    "        ids_mapping = pd.DataFrame({\"freebase_id\": freebase_ids, \"imdb_id\": imdb_ids, \"metacritic_id\": metacritic_ids})\n",
    "\n",
    "        # if duplicates then set imdb and metacritic ids to None\n",
    "        duplicates = ids_mapping[\"freebase_id\"].duplicated(keep=False)\n",
    "        ids_mapping.loc[duplicates, [\"imdb_id\", \"metacritic_id\"]] = None\n",
    "\n",
    "        # removing duplicates\n",
    "        ids_mapping = ids_mapping.drop_duplicates(subset=[\"freebase_id\"])\n",
    "\n",
    "        # Ensuring one-to-one correspondence\n",
    "        if not ids_mapping[\"freebase_id\"].is_unique:\n",
    "            # print duplicates\n",
    "            raise ValueError(\"Duplicate freebase_ids found in ids_mapping.\")\n",
    "\n",
    "        # Merging and updating the DataFrame\n",
    "        batch_updated = batch.merge(ids_mapping, on=\"freebase_id\", how=\"left\")\n",
    "        new_movies.iloc[i:i+batch_size] = batch_updated\n",
    "\n",
    "    return new_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('data/preprocessed/movie.metadata.preprocessed.tsv', sep='\\t')\n",
    "\n",
    "cmu_movies = add_imdb_and_metacritics_ids(movies, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_movies.to_csv('data/processed/cmu_movies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metacritic Critics Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics_movie(metacriticID):\n",
    "\n",
    "    url = f\"https://www.metacritic.com/{metacriticID}/critic-reviews\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    script_tag = soup.find('script', string=lambda t: t and 'window.__NUXT__' in t)\n",
    "\n",
    "    # Define a regular expression pattern to match objects with specific attributes\n",
    "    pattern = r\"\\{[^{}]*reviewedProduct:\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}[^{}]*\\}\"\n",
    "\n",
    "    if not script_tag:\n",
    "        return None\n",
    "    \n",
    "    # Find all matches of the pattern in the input text\n",
    "    matches = re.findall(pattern, script_tag.text, re.DOTALL)\n",
    "    \n",
    "    def extract_info(data_string):\n",
    "        \n",
    "        # Regular expressions for score (next to metaScore), author, and publicationName\n",
    "        metascore_pattern = r\"criticScoreSummary:\\{[^\\}]*score:(\\d+)\"\n",
    "        score_pattern = r'score:([a-zA-Z]|\\d+),\\s*metaScore'\n",
    "        author_pattern = r'author:\"([^\"]+)\"'\n",
    "        publication_name_pattern = r'publicationName:\"([^\"]+)\"'\n",
    "\n",
    "        # Extracting score\n",
    "        score_match = re.search(score_pattern, data_string)\n",
    "        if score_match:\n",
    "            score = score_match.group(1)\n",
    "            if score.isalpha():\n",
    "                score = 0\n",
    "            else:\n",
    "                score = int(score)\n",
    "        else:\n",
    "            score = None\n",
    "\n",
    "        # Extracting author\n",
    "        author_match = re.search(author_pattern, data_string)\n",
    "        author = author_match.group(1) if author_match else None\n",
    "\n",
    "        # Extracting publicationName\n",
    "        publication_name_match = re.search(publication_name_pattern, data_string)\n",
    "        publication_name = publication_name_match.group(1) if publication_name_match else None\n",
    "\n",
    "        # Extracting metascore\n",
    "        metascore_match = re.search(metascore_pattern, data_string)\n",
    "        metascore = metascore_match.group(1) if metascore_match else None\n",
    "\n",
    "        return score, author, publication_name, metascore\n",
    "        \n",
    "    reviews_data = {\"publisher\": [], \"author\": [], \"metacritic_rating\": [], \"metascore\": []}\n",
    "    \n",
    "    for review in matches:\n",
    "\n",
    "        score, author, publisher, metascore = extract_info(review)\n",
    "\n",
    "        reviews_data[\"publisher\"].append(publisher)\n",
    "        reviews_data[\"author\"].append(author)\n",
    "        reviews_data[\"metacritic_rating\"].append(score)\n",
    "        reviews_data[\"metascore\"].append(metascore)\n",
    "\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics(movies_df, save_step=250, filepath=\"data/external/metacritic_reviews.csv\"):\n",
    "    if not os.path.exists(\"data/scrap/metacritic_reviews.csv\"):\n",
    "        metacritic_reviews = pd.DataFrame(columns=[\"publisher\", \"author\", \"metacritic_rating\", \"metascore\", \"metacritic_id\"])\n",
    "    else:\n",
    "        metacritic_reviews = pd.read_csv(\"data/scrap/metacritic_reviews.csv\")\n",
    "\n",
    "    already_scraped_movies_ids = metacritic_reviews[\"metacritic_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    \n",
    "    movies_df = movies_df.loc[~movies_df[\"metacritic_id\"].isin(already_scraped_movies_ids)]\n",
    "\n",
    "    movies_df = movies_df.loc[movies_df[\"metacritic_id\"].notna()]\n",
    "\n",
    "    initial_len = len(metacritic_reviews)\n",
    "\n",
    "    for i, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        metacritic_id = row[\"metacritic_id\"]\n",
    "\n",
    "        if not metacritic_id:\n",
    "            continue\n",
    "\n",
    "        reviews = scrap_metacritics_movie(metacritic_id)\n",
    "\n",
    "        if reviews is None:\n",
    "            continue\n",
    "\n",
    "        reviews[\"metacritic_id\"] = row[\"metacritic_id\"]\n",
    "\n",
    "        metacritic_reviews = pd.concat([metacritic_reviews, reviews], ignore_index=True)\n",
    "\n",
    "        if i % save_step == 0:\n",
    "            metacritic_reviews.to_csv(\"data/scrap/metacritic_reviews.csv\", index=False)\n",
    "            print(\"Saved {} new reviews\".format(len(metacritic_reviews) - initial_len))\n",
    "            initial_len = len(metacritic_reviews)\n",
    "\n",
    "    metacritic_reviews.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"data/processed/cmu_movies.csv\")\n",
    "\n",
    "scrap_metacritics(movies, save_step=250, filepath=\"data/external/metacritic_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards_nominations_batch(freebase_ids):\n",
    "    formatted_ids = ' '.join(f'\"{id_}\"' for id_ in freebase_ids)\n",
    "    sparql_query_awards = f\"\"\"\n",
    "    SELECT ?item ?movieLabel ?movieFreebaseID ?awardLabel WHERE {{\n",
    "    VALUES ?movieFreebaseID {{ {formatted_ids} }}\n",
    "    ?item wdt:P646 ?movieFreebaseID .\n",
    "    OPTIONAL {{ \n",
    "      ?item wdt:P166 ?award .\n",
    "      ?award rdfs:label ?awardLabel .\n",
    "      FILTER(LANG(?awardLabel) = \"en\")\n",
    "    }}\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "  }}\n",
    "    \"\"\"\n",
    "    encoded_query_awards = urllib.parse.quote(sparql_query_awards)\n",
    "\n",
    "    sparql_query_nominations = f\"\"\"\n",
    "    SELECT ?item ?movieLabel ?movieFreebaseID ?nominationLabel WHERE {{\n",
    "    VALUES ?movieFreebaseID {{ {formatted_ids} }}\n",
    "    ?item wdt:P646 ?movieFreebaseID .\n",
    "    OPTIONAL {{ \n",
    "      ?item wdt:P1411 ?nomination .\n",
    "      ?nomination rdfs:label ?nominationLabel .\n",
    "      FILTER(LANG(?nominationLabel) = \"en\")\n",
    "    }}\n",
    "    SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }}\n",
    "  }}\n",
    "    \"\"\"\n",
    "    encoded_query_nominations = urllib.parse.quote(sparql_query_nominations)\n",
    "\n",
    "    url = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query={}\"\n",
    "    url_awards = url.format(encoded_query_awards)\n",
    "    url_nominations = url.format(encoded_query_nominations)\n",
    "    \n",
    "    # Fetch awards\n",
    "    response_awards = requests.get(url_awards)\n",
    "    if response_awards.status_code == 200:\n",
    "      data_awards = response_awards.json()\n",
    "\n",
    "      results_awards = [{\n",
    "          'freebase_id': item['movieFreebaseID']['value'],\n",
    "          'type': 'award',\n",
    "          'name': item['awardLabel']['value']\n",
    "      } for item in data_awards['results']['bindings'] if 'awardLabel' in item]\n",
    "    else:\n",
    "      results_awards = []\n",
    "\n",
    "    # Fetch nominations\n",
    "    response_nominations = requests.get(url_nominations)\n",
    "\n",
    "    if response_nominations.status_code == 200:\n",
    "      data_nominations = response_nominations.json()\n",
    "      results_nominations = [{\n",
    "          'freebase_id': item['movieFreebaseID']['value'],\n",
    "          'type': 'nomination',\n",
    "          'name': item['nominationLabel']['value']\n",
    "      } for item in data_nominations['results']['bindings'] if 'nominationLabel' in item]\n",
    "    else:\n",
    "      results_nominations = []\n",
    "\n",
    "    # Combine results and create DataFrame\n",
    "    combined_results = results_awards + results_nominations\n",
    "    return pd.DataFrame(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_awards_nominations(movies, batch_size=250):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(movies), batch_size), total=len(movies)//batch_size):\n",
    "        batch = movies[i:i+batch_size]\n",
    "        freebase_ids = batch['freebase_id'].tolist()\n",
    "        results.append(get_awards_nominations_batch(freebase_ids))\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awards - IMDb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_awards_movie(metacriticID):\n",
    "    url = 'https://www.imdb.com/title/{}/awards/'.format(metacriticID)    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    div = soup.find_all('div', attrs={'data-testid':\"awards-signpost\"})\n",
    "    if len(div) == 0:\n",
    "        return 0, 0\n",
    "    awards = div[0].find_all('div', class_=\"ipc-signpost__text\")[0].get_text(strip=True)\n",
    "    # extract the number of awards\n",
    "    # Structure of the text: \"N wins & M nominations.\"\n",
    "    wins = 0\n",
    "    nominations = 0\n",
    "    if len(awards.split('&')) == 2:\n",
    "        wins = int(awards.split('&')[0].split()[0])\n",
    "        nominations = int(awards.split('&')[1].split()[0])\n",
    "    elif \"wins\" in awards:\n",
    "        wins = int(awards.split()[0])\n",
    "    elif \"nominations\" in awards:\n",
    "        nominations = int(awards.split()[0])\n",
    "    return wins, nominations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_awards(movies_df, save_step=250):\n",
    "    if not os.path.exists(\"data/external/imdb_awards.csv\"):\n",
    "        imdb_awards = pd.DataFrame(columns=[\"freebase_id\", \"nominations\", \"wins\"])\n",
    "    else:\n",
    "        imdb_awards = pd.read_csv(\"data/external/imdb_awards.csv\")\n",
    "\n",
    "    already_scraped_movies_ids = imdb_awards[\"freebase_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    movies_df = movies_df.loc[~movies_df[\"freebase_id\"].isin(already_scraped_movies_ids)].reset_index(drop=True)\n",
    "    \n",
    "    initial_len = len(imdb_awards)\n",
    "\n",
    "    for i, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        imdb_id = row[\"imdb_id\"]\n",
    "        if not imdb_id:\n",
    "            print(row)\n",
    "            continue\n",
    "\n",
    "        wins, nominations = scrap_awards_movie(imdb_id)\n",
    "\n",
    "        award = pd.DataFrame({\"freebase_id\": [row[\"freebase_id\"]], \"nominations\": [nominations], \"wins\": [wins]})\n",
    "\n",
    "        imdb_awards = pd.concat([imdb_awards, award], ignore_index=True)\n",
    "        if i % save_step == 0:\n",
    "            imdb_awards.to_csv(\"data/external/imdb_awards.csv\", index=False)\n",
    "            print(\"Saved {} new awards\".format(len(imdb_awards) - initial_len))\n",
    "            initial_len = len(imdb_awards)\n",
    "\n",
    "    imdb_awards.to_csv(\"data/external/imdb_awards.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"data/processed/cmu_movies.csv\", sep=\"\\t\")\n",
    "movies = movies.loc[(movies['freebase_id'].notnull()) & (movies['imdb_id'].notnull()) & (movies['metacritic_id'].notnull())] \n",
    "scrap_awards(movies, save_step=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.5",
   "language": "sage",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
