{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDS(imdb_ids=[], freebase_ids=[]):\n",
    "    '''\n",
    "        Get the imdb_id, freebase_id and metacritic_id from the wikidata database\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        imdb_ids : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        imdb_id : str\n",
    "            The imdb id of the movie\n",
    "        freebase_id : str\n",
    "            The freebase id of the movie\n",
    "        metacritic_id : str\n",
    "            The metacritic id of the movie\n",
    "    '''\n",
    "    if len(imdb_ids) > 0:\n",
    "        imdb_ids_string = \" \".join(f'\"{id_}\"' for id_ in imdb_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?imdbId {{ {imdb_ids_string} }}\n",
    "            ?item wdt:P345 ?imdbId .\n",
    "            OPTIONAL {{ ?item wdt:P646 ?freebaseId }}\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    elif len(freebase_ids) > 0:\n",
    "        freebase_ids_string = \" \".join(f'\"{id_}\"' for id_ in freebase_ids)\n",
    "\n",
    "        query = f\"\"\"\n",
    "            SELECT ?item ?freebaseId ?metacriticId ?imdbId WHERE {{\n",
    "            VALUES ?freebaseId {{ {freebase_ids_string} }}\n",
    "            ?item wdt:P646 ?freebaseId .\n",
    "            OPTIONAL {{ ?item wdt:P1712 ?metacriticId }}\n",
    "            OPTIONAL {{ ?item wdt:P345 ?imdbId }}\n",
    "            }}\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        return None, None, None\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    query_url = f\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query={encoded_query}\"\n",
    "\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = response.json()\n",
    "\n",
    "    if len(data[\"results\"][\"bindings\"]) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    data = data[\"results\"][\"bindings\"]\n",
    "    \n",
    "    imdb_ids = []\n",
    "    freebase_ids = []\n",
    "    metacritic_ids = []\n",
    "\n",
    "    for item in data:\n",
    "        if \"freebaseId\" in item:\n",
    "            freebase_ids.append(item[\"freebaseId\"][\"value\"])\n",
    "        else:\n",
    "            freebase_ids.append(None)\n",
    "        \n",
    "        if \"metacriticId\" in item:\n",
    "            metacritic_ids.append(item[\"metacriticId\"][\"value\"])\n",
    "        else:\n",
    "            metacritic_ids.append(None)\n",
    "\n",
    "        if \"imdbId\" in item:\n",
    "            imdb_ids.append(item[\"imdbId\"][\"value\"])\n",
    "        else:\n",
    "            imdb_ids.append(None)\n",
    "\n",
    "    return imdb_ids, freebase_ids, metacritic_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movies():\n",
    "    url = \"https://www.imdb.com/search/title/?groups=top_1000&sort=user_rating,desc&start={start}\"\n",
    "\n",
    "    data = {\n",
    "        \"imdb_id\": [], \n",
    "        \"title\": [],\n",
    "        \"year\": [],\n",
    "        \"duration\": [],\n",
    "        \"genres\": [],\n",
    "        \"number_of_ratings\": [],\n",
    "        \"rating\": [],\n",
    "        \"director\": [],\n",
    "        \"description\": [],\n",
    "        \"freebase_id\": [],\n",
    "        \"metacritic_id\": [],\n",
    "        \"metascore\": [],\n",
    "    }\n",
    "\n",
    "    pbar = tqdm(total=1000)\n",
    "\n",
    "    while True:\n",
    "        start = len(data[\"title\"]) + 1\n",
    "\n",
    "        response = requests.get(url.format(start=start))\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        movies = soup.find_all(class_=\"lister-item-content\")\n",
    "\n",
    "        if not movies or len(movies) == 0:\n",
    "            break\n",
    "\n",
    "        for movie in movies:\n",
    "            pbar.update(1)\n",
    "            imdb_id = movie.find(\"a\")[\"href\"].split(\"/\")[2]\n",
    "            data[\"imdb_id\"].append(imdb_id)\n",
    "            data[\"title\"].append(movie.find(\"a\").get_text())\n",
    "\n",
    "            year_text = movie.find(class_=\"lister-item-year\").get_text().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "            year_match = re.search(r'\\d{4}', year_text)\n",
    "\n",
    "            if year_match:\n",
    "                year = year_match.group()\n",
    "            else:\n",
    "                year = None\n",
    "        \n",
    "            data[\"year\"].append(year)\n",
    "            data[\"duration\"].append(movie.find(class_=\"runtime\").get_text())\n",
    "            data[\"genres\"].append(movie.find(class_=\"genre\").get_text().strip())\n",
    "            data[\"number_of_ratings\"].append(\n",
    "                movie.find(class_=\"sort-num_votes-visible\").find_all(\"span\")[1].get_text()\n",
    "            )\n",
    "            data[\"rating\"].append(movie.find(class_=\"ratings-imdb-rating\").get_text().strip())\n",
    "            data[\"director\"].append(movie.find_all(\"p\")[2].find(\"a\").get_text())\n",
    "            data[\"description\"].append(movie.find_all(\"p\", class_=\"text-muted\")[-1].get_text().strip())\n",
    "            metascore = movie.find(\"span\", class_=\"metascore\")\n",
    "            if metascore:\n",
    "                data[\"metascore\"].append(int(metascore.get_text().strip()))\n",
    "            else:\n",
    "                data[\"metascore\"].append(None)\n",
    "\n",
    "            _, freebase_ids, metacritic_ids = get_IDS(imdb_ids=[imdb_id])\n",
    "\n",
    "            freebase_id = freebase_ids[0] if freebase_ids else None\n",
    "            metacritic_id = metacritic_ids[0] if metacritic_ids else None\n",
    "\n",
    "            data[\"freebase_id\"].append(freebase_id)\n",
    "            data[\"metacritic_id\"].append(metacritic_id)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    df[\"rating\"] = df[\"rating\"].astype(float)\n",
    "    df['number_of_ratings'] = df['number_of_ratings'].str.replace(',', '').astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = scrap_movies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.to_csv(\"data/scrap/top1000_IMDB_movies.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_reviews(movieID, max_pages=None):\n",
    "    url = (\n",
    "        \"https://www.imdb.com/title/{movieID}/reviews/_ajax?paginationKey={}\"\n",
    "    )\n",
    "    key = \"\"\n",
    "    data = {\"imdb_id\": [], \"review\": [], \"rating\": [], \"date\": [], \"user\": []}\n",
    "\n",
    "    response = requests.get(\"https://www.imdb.com/title/{movieID}/reviews\".format(movieID = movieID))\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    total_reviews = int(soup.find(class_=\"lister\").find(class_=\"header\").find(\"span\").get_text().split()[0].replace(',', ''))\n",
    "    \n",
    "    pbar2 = tqdm(total=total_reviews, position=1, leave=True)\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        if max_pages and i > max_pages:\n",
    "            break\n",
    "        response = requests.get(url.format(key, movieID = movieID))\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        for review, rating, date, user in zip(\n",
    "            soup.find_all(class_=\"text show-more__control\"), soup.find_all(class_=\"rating-other-user-rating\"), soup.find_all(class_=\"review-date\"), soup.find_all(class_=\"display-name-link\")\n",
    "        ):\n",
    "            data[\"imdb_id\"].append(movieID)\n",
    "            data[\"review\"].append(review.get_text())\n",
    "            data[\"rating\"].append(rating.find(\"span\").get_text(strip=True))\n",
    "            data[\"date\"].append(date.get_text(strip=True))\n",
    "            data[\"user\"].append(user.find(\"a\")[\"href\"].split(\"/\")[2])\n",
    "\n",
    "        # Find the pagination key\n",
    "        pagination_key = soup.find(\"div\", class_=\"load-more-data\")\n",
    "        if not pagination_key:\n",
    "            break\n",
    "        \n",
    "        # Update the `key` variable in-order to scrape more reviews\n",
    "        key = pagination_key[\"data-key\"]\n",
    "        \n",
    "        pbar2.update(25)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df['rating'] = df['rating'].astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_reviews(movies, max_pages=None, reviews_threshold=10000):\n",
    "    reviews = pd.DataFrame()\n",
    "    \n",
    "    files = os.listdir(\"data/reviews\")\n",
    "\n",
    "    already_scraped_movies = pd.Series()\n",
    "\n",
    "    if len(files) > 0:\n",
    "        for file in files:\n",
    "            file_reviews = pd.read_csv(\"data/reviews/{}\".format(file))\n",
    "            already_scraped_movies = pd.concat([already_scraped_movies, file_reviews['movie']])\n",
    "\n",
    "    already_scraped_movies = already_scraped_movies.unique()\n",
    "\n",
    "    print(\"Already scraped {}/{} movies\".format(len(already_scraped_movies), len(movies)))\n",
    "\n",
    "    movies = movies[~movies[\"id\"].isin(already_scraped_movies)]\n",
    "\n",
    "    if len(movies) == 0:\n",
    "        print(\"All movies have already been scraped\")\n",
    "        return None\n",
    "\n",
    "    pbar1 = tqdm(total=len(movies), position=0, leave=True)\n",
    "    \n",
    "\n",
    "    for movieID in movies[\"imdb_id\"]:\n",
    "\n",
    "        print('Scraping reviews for movie {}'.format(movies[movies['imdb_id'] == movieID]['title'].values[0]))\n",
    "\n",
    "        reviews = pd.concat([reviews, scrap_movie_reviews(movieID, max_pages=max_pages)])\n",
    "\n",
    "        if len(reviews) > reviews_threshold:\n",
    "            # create a file \"review\" + i + \".csv\" with i the number of files in the reviews folder\n",
    "            files = os.listdir(\"data/reviews\")\n",
    "            file_number = len(files) + 1\n",
    "            reviews.to_csv(\"data/reviews/reviews{}.csv\".format(file_number), index=False)\n",
    "            \n",
    "            print(\"Scraped {} reviews\".format(len(reviews)))\n",
    "            print(\"Saved to data/reviews/reviews{}.csv\".format(file_number))\n",
    "            \n",
    "            # reset the reviews dataframe\n",
    "            reviews = pd.DataFrame()\n",
    "        pbar1.update(1)\n",
    "\n",
    "    # append the reviews to the csv file\n",
    "    if len(reviews) > 0:\n",
    "        files = os.listdir(\"data/reviews\")\n",
    "        file_number = len(files) + 1\n",
    "        reviews.to_csv(\"data/reviews/reviews{}.csv\".format(file_number), index=False)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_reviews(movies, max_pages=None, reviews_threshold=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_cast(movieID):\n",
    "    url = f\"https://www.imdb.com/title/{movieID}/fullcredits\"\n",
    "\n",
    "    data = {\"imdb_id\": [], \"actor\": [], \"character\": []}\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    castlist = soup.find_all(\"table\", class_=\"cast_list\")[0].find_all(\"tr\")\n",
    "\n",
    "    castlist = castlist[1:]\n",
    "\n",
    "    for cast in castlist:\n",
    "        if not(\"odd\" in cast.attrs.get(\"class\", []) or \"even\" in cast.attrs.get(\"class\", [])):\n",
    "            break\n",
    "\n",
    "        data[\"imdb_id\"].append(movieID)\n",
    "        data[\"actor\"].append(cast.find_all(\"td\")[1].find(\"a\").get_text(strip=True))\n",
    "        data[\"character\"].append(cast.find(\"td\", class_=\"character\").get_text(strip=True))\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_casts(movies):\n",
    "    casts = pd.DataFrame()\n",
    "\n",
    "    for movieID in tqdm(movies[\"imdb_id\"]):\n",
    "        casts = pd.concat([casts, scrap_movie_cast(movieID)])\n",
    "    return casts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts = scrap_casts(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts.to_csv(\"data/casts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_movie_summary(imdb_id):\n",
    "    url = f\"https://www.imdb.com/title/{imdb_id}/plotsummary\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    synopsis = soup.find_all('section', class_=\"ipc-page-section ipc-page-section--base\")[1].find(class_=\"ipc-metadata-list-item__content-container\")\n",
    "    if synopsis:\n",
    "        synopsis = synopsis.get_text(strip=True)\n",
    "\n",
    "    return synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_summaries(movies):\n",
    "    summaries = {\n",
    "        \"imdb_id\": [],\n",
    "        \"summary\": []\n",
    "    }\n",
    "\n",
    "    for imdb_id in tqdm(movies[\"imdb_id\"]):\n",
    "        summary = scrap_movie_summary(imdb_id)\n",
    "\n",
    "        if summary:\n",
    "            summaries[\"imdb_id\"].append(imdb_id)\n",
    "            summaries[\"summary\"].append(summary)\n",
    "\n",
    "    df = pd.DataFrame(summaries)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = scrap_summaries(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries.to_csv(\"data/summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critics Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics_movie(metacriticID):\n",
    "\n",
    "    url = f\"https://www.metacritic.com/{metacriticID}/critic-reviews\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    script_tag = soup.find('script', string=lambda t: t and 'window.__NUXT__' in t)\n",
    "\n",
    "    # Define a regular expression pattern to match objects with specific attributes\n",
    "    pattern = r\"\\{[^{}]*reviewedProduct:\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}[^{}]*\\}\"\n",
    "\n",
    "    if not script_tag:\n",
    "        return None\n",
    "    \n",
    "    # Find all matches of the pattern in the input text\n",
    "    matches = re.findall(pattern, script_tag.text, re.DOTALL)\n",
    "    \n",
    "    def extract_info(data_string):\n",
    "        \n",
    "        # Regular expressions for score (next to metaScore), author, and publicationName\n",
    "        metascore_pattern = r\"criticScoreSummary:\\{[^\\}]*score:(\\d+)\"\n",
    "        score_pattern = r'score:([a-zA-Z]|\\d+),\\s*metaScore'\n",
    "        author_pattern = r'author:\"([^\"]+)\"'\n",
    "        publication_name_pattern = r'publicationName:\"([^\"]+)\"'\n",
    "\n",
    "        # Extracting score\n",
    "        score_match = re.search(score_pattern, data_string)\n",
    "        if score_match:\n",
    "            score = score_match.group(1)\n",
    "            if score.isalpha():\n",
    "                score = 0\n",
    "            else:\n",
    "                score = int(score)\n",
    "        else:\n",
    "            score = None\n",
    "\n",
    "        # Extracting author\n",
    "        author_match = re.search(author_pattern, data_string)\n",
    "        author = author_match.group(1) if author_match else None\n",
    "\n",
    "        # Extracting publicationName\n",
    "        publication_name_match = re.search(publication_name_pattern, data_string)\n",
    "        publication_name = publication_name_match.group(1) if publication_name_match else None\n",
    "\n",
    "        # Extracting metascore\n",
    "        metascore_match = re.search(metascore_pattern, data_string)\n",
    "        metascore = metascore_match.group(1) if metascore_match else None\n",
    "\n",
    "        return score, author, publication_name, metascore\n",
    "        \n",
    "    reviews_data = {\"publisher\": [], \"author\": [], \"metacritic_rating\": [], \"metascore\": []}\n",
    "    \n",
    "    for review in matches:\n",
    "\n",
    "        score, author, publisher, metascore = extract_info(review)\n",
    "\n",
    "        reviews_data[\"publisher\"].append(publisher)\n",
    "        reviews_data[\"author\"].append(author)\n",
    "        reviews_data[\"metacritic_rating\"].append(score)\n",
    "        reviews_data[\"metascore\"].append(metascore)\n",
    "\n",
    "    reviews_df = pd.DataFrame(reviews_data)\n",
    "\n",
    "    return reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_metacritics(movies_df, save_step=250):\n",
    "    if not os.path.exists(\"data/scrap/metacritic_reviews.csv\"):\n",
    "        metacritic_reviews = pd.DataFrame(columns=[\"publisher\", \"author\", \"metacritic_rating\", \"metascore\", \"metacritic_id\"])\n",
    "    else:\n",
    "        metacritic_reviews = pd.read_csv(\"data/scrap/metacritic_reviews.csv\")\n",
    "\n",
    "    already_scraped_movies_ids = metacritic_reviews[\"metacritic_id\"].unique()\n",
    "\n",
    "    # filter movies that have already been scraped\n",
    "    \n",
    "    movies_df = movies_df.loc[~movies_df[\"metacritic_id\"].isin(already_scraped_movies_ids)]\n",
    "\n",
    "    movies_df = movies_df.loc[movies_df[\"metacritic_id\"].notna()]\n",
    "\n",
    "    initial_len = len(metacritic_reviews)\n",
    "\n",
    "    for i, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        metacritic_id = row[\"metacritic_id\"]\n",
    "\n",
    "        if not metacritic_id:\n",
    "            continue\n",
    "\n",
    "        reviews = scrap_metacritics_movie(metacritic_id)\n",
    "\n",
    "        if reviews is None:\n",
    "            continue\n",
    "\n",
    "        reviews[\"metacritic_id\"] = row[\"metacritic_id\"]\n",
    "\n",
    "        metacritic_reviews = pd.concat([metacritic_reviews, reviews], ignore_index=True)\n",
    "\n",
    "        if i % save_step == 0:\n",
    "            metacritic_reviews.to_csv(\"data/scrap/metacritic_reviews.csv\", index=False)\n",
    "            print(\"Saved {} new reviews\".format(len(metacritic_reviews) - initial_len))\n",
    "            initial_len = len(metacritic_reviews)\n",
    "\n",
    "    metacritic_reviews.to_csv(\"data/scrap/metacritic_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"data/scrap/top1000_IMDB_movies.csv\")\n",
    "\n",
    "scrap_metacritics(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match IMDB, MetaCritic and Freebase IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_imdb_and_metacritics_ids(movies, batch_size=100):\n",
    "\n",
    "    new_movies = movies.copy()\n",
    "\n",
    "    # Initializing imdb_id and metacritic_id columns if they don't exist\n",
    "    if 'imdb_id' not in new_movies.columns:\n",
    "        new_movies['imdb_id'] = None\n",
    "    if 'metacritic_id' not in new_movies.columns:\n",
    "        new_movies['metacritic_id'] = None\n",
    "\n",
    "    for i in tqdm(range(0, len(new_movies), batch_size)):\n",
    "        batch = new_movies.iloc[i:i+batch_size]\n",
    "\n",
    "        # drop columns imdb_id and metacritic_id\n",
    "        batch = batch.drop(columns=[\"imdb_id\", \"metacritic_id\"])\n",
    "\n",
    "        imdb_ids, freebase_ids, metacritic_ids = get_IDS(freebase_ids=batch[\"freebase_id\"].values)\n",
    "\n",
    "        ids_mapping = pd.DataFrame({\"freebase_id\": freebase_ids, \"imdb_id\": imdb_ids, \"metacritic_id\": metacritic_ids})\n",
    "\n",
    "        # if duplicates then set imdb and metacritic ids to None\n",
    "        duplicates = ids_mapping[\"freebase_id\"].duplicated(keep=False)\n",
    "        ids_mapping.loc[duplicates, [\"imdb_id\", \"metacritic_id\"]] = None\n",
    "\n",
    "        # removing duplicates\n",
    "        ids_mapping = ids_mapping.drop_duplicates(subset=[\"freebase_id\"])\n",
    "\n",
    "        # Ensuring one-to-one correspondence\n",
    "        if not ids_mapping[\"freebase_id\"].is_unique:\n",
    "            # print duplicates\n",
    "            raise ValueError(\"Duplicate freebase_ids found in ids_mapping.\")\n",
    "\n",
    "        # Merging and updating the DataFrame\n",
    "        batch_updated = batch.merge(ids_mapping, on=\"freebase_id\", how=\"left\")\n",
    "        new_movies.iloc[i:i+batch_size] = batch_updated\n",
    "\n",
    "    return new_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_movies = pd.read_csv('data/movie.metadata.tsv', sep='\\t', header=None, names=['wikipedia_id', 'freebase_id', 'title', 'release_date', 'revenue', 'runtime', 'languages', 'countries', 'genres'])\n",
    "\n",
    "new_cmu_movies = add_imdb_and_metacritics_ids(cmu_movies, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cmu_movies.to_csv(\"data/cmu_movies.csv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap Metacritic for the new CMU dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cmu_movies = pd.read_csv(\"data/cmu_movies.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 389/2203 [11:32<43:11,  1.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 26 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 557/2203 [14:53<35:27,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3238 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 770/2203 [18:33<25:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4141 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 1032/2203 [22:52<19:24,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4697 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1158/2203 [24:50<20:35,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2151 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 1353/2203 [28:05<14:02,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3648 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1703/2203 [33:40<08:56,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6404 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1915/2203 [37:10<04:31,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3958 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 2003/2203 [38:38<03:23,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1696 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 2032/2203 [39:07<03:06,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 611 new reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2203/2203 [42:01<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "scrap_metacritics(new_cmu_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrap_metacritics(new_cmu_movies.loc[~(new_cmu_movies[\"imdb_id\"].isna()) & ~(new_cmu_movies[\"metacritic_id\"].isna())], save_step=250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
